{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出没有被处理的样本\n",
    "import json\n",
    "\n",
    "# 加载JSON文件\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# 保存数据到JSON文件\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "8721\n"
     ]
    }
   ],
   "source": [
    "# 不完整的处理后的样本\n",
    "file1_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/combined_data.json' \n",
    "# 原本的样本\n",
    "file2_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/fgeo_1_10_gpt_trans.json'\n",
    "output_sorted_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/sort_combined_data.json'\n",
    "output_missing_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/unpre_data.json'\n",
    "\n",
    "# 加载两个JSON文件\n",
    "data1 = load_json(file1_path)\n",
    "data2 = load_json(file2_path)\n",
    "print(len(data1))\n",
    "print(len((data2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存排序数据到 /mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/sort_combined_data.json\n",
      "已保存缺失数据到 /mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/unpre_data.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 文件路径替换为你的文件路径\n",
    "# 不完整的处理后的样本\n",
    "file1_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/combined_data.json' \n",
    "# 原本的样本\n",
    "file2_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/fgeo_1_10_gpt_trans.json'\n",
    "output_sorted_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/sort_combined_data.json'\n",
    "output_missing_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/unpre_data.json'\n",
    "\n",
    "# 加载两个JSON文件\n",
    "data1 = load_json(file1_path)\n",
    "data2 = load_json(file2_path)\n",
    "\n",
    "# 为第一个文件的样本创建ID映射\n",
    "data1_dict = {item['id']: item for item in data1}\n",
    "\n",
    "# 按照第二个文件中的顺序重新排列第一个文件中的样本\n",
    "sorted_data1 = [data1_dict[item['id']] for item in data2 if item['id'] in data1_dict]\n",
    "\n",
    "# 找到第一个文件中但第二个文件中不存在的样本\n",
    "missing_data = [item for item in data2 if item['id'] not in set(item['id'] for item in data1)]\n",
    "\n",
    "# 保存结果\n",
    "save_json(sorted_data1, output_sorted_path)\n",
    "save_json(missing_data, output_missing_path)\n",
    "\n",
    "print(f'已保存排序数据到 {output_sorted_path}')\n",
    "print(f'已保存缺失数据到 {output_missing_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 加载JSON文件\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# 保存数据到JSON文件\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# 文件路径替换为你的文件路径\n",
    "# 不完整的处理后的样本\n",
    "file1_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/combined_data.json' \n",
    "# 原本的样本\n",
    "file2_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/fgeo_1_10_gpt_trans.json'\n",
    "output_sorted_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/sort_combined_data.json'\n",
    "output_missing_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/unpre_data.json'\n",
    "\n",
    "# 加载两个JSON文件\n",
    "data1 = load_json(file1_path)\n",
    "data2 = load_json(file2_path)\n",
    "\n",
    "# 为第一个文件的样本创建ID映射\n",
    "data1_dict = {item['id']: item for item in data1}\n",
    "\n",
    "# 按照第二个文件中的顺序重新排列第一个文件中的样本\n",
    "sorted_data1 = [data1_dict[item['id']] for item in data2 if item['id'] in data1_dict]\n",
    "\n",
    "# 找到第一个文件中但第二个文件中不存在的样本\n",
    "missing_data = [item for item in data2 if item['id'] not in set(item['id'] for item in data1)]\n",
    "\n",
    "# 保存结果\n",
    "save_json(sorted_data1, output_sorted_path)\n",
    "save_json(missing_data, output_missing_path)\n",
    "\n",
    "print(f'已保存排序数据到 {output_sorted_path}')\n",
    "print(f'已保存缺失数据到 {output_missing_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并两个json文件\n",
    "file1_path = ''\n",
    "file2_path = ''\n",
    "save_path = ''\n",
    "\n",
    "data1 = load_json(file1_path)\n",
    "data2 = load_json(file2_path)\n",
    "conbined_data = data1 + data2\n",
    "\n",
    "save_json(conbined_data, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查预测样本json文件的完整性\n",
    "import json\n",
    "\n",
    "def compare_json_files(file1, file2):\n",
    "    # 读取第一个JSON文件\n",
    "    with open(file1, 'r', encoding='utf-8') as f1:\n",
    "        data1 = json.load(f1)\n",
    "\n",
    "    # 读取第二个JSON文件\n",
    "    with open(file2, 'r', encoding='utf-8') as f2:\n",
    "        data2 = json.load(f2)\n",
    "\n",
    "    # 用字典创建一个快速索引，以便通过id快速访问\n",
    "    data1_index = {item['id']: item for item in data1}\n",
    "    data2_index = {item['id']: item for item in data2}\n",
    "\n",
    "    # 检测样本\"conversations\"长度是否相同\n",
    "    for id_ in data1_index:\n",
    "        if id_ in data2_index:\n",
    "            conv_len1 = len(data1_index[id_].get(\"conversations\", []))\n",
    "            conv_len2 = len(data2_index[id_].get(\"conversations\", []))\n",
    "\n",
    "            if conv_len1*2 != conv_len2:\n",
    "                print(f\"Difference found in id {id_}: Length in file1 is {conv_len1}, in file2 is {conv_len2}\")\n",
    "        else:\n",
    "            print(f\"id {id_} not found in file2\")\n",
    "\n",
    "    # 检查file2中有，而file1中没有的id\n",
    "    for id_ in data2_index:\n",
    "        if id_ not in data1_index:\n",
    "            print(f\"id {id_} not found in file1\")\n",
    "file1_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/fgeo_1_10_gpt_results.json'\n",
    "file2_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/filter_geoqa+_1_10.json'\n",
    "# 示例使用\n",
    "compare_json_files(file1_path, file2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤掉整张图片的描述\n",
    "import json\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate the Intersection over Union (IoU) of two bounding boxes.\"\"\"\n",
    "    x_left = max(box1[0], box2[0])\n",
    "    y_top = max(box1[1], box2[1])\n",
    "    x_right = min(box1[2], box2[2])\n",
    "    y_bottom = min(box1[3], box2[3])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "    return iou\n",
    "\n",
    "file_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/fgeo_1_10_bbox.json'\n",
    "with open(file_path, 'r') as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "reference_box = [0, 0, 1, 1]  # The reference box to compare against\n",
    "for sample_data in data1:\n",
    "    for conversation in sample_data[\"conversations\"]:\n",
    "        for attr in [\"<CAPTION>\", \"<REASONING>\"]:\n",
    "            if conversation[attr] is not None:\n",
    "                for item in conversation[attr]:\n",
    "                    iou = calculate_iou(item[\"bbox\"], reference_box)\n",
    "                    if iou > 0.8:\n",
    "                        item[\"bbox\"] = None\n",
    "\n",
    "# Output the modified sample_data\n",
    "with open('/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/test.json', 'w') as f:\n",
    "    json.dump(data1, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计有效bbox个数\n",
    "file_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/test.json'\n",
    "with open(file_path, 'r') as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "count = 0\n",
    "for sample_data in data1:\n",
    "    for conversation in sample_data[\"conversations\"]:\n",
    "        for attr in [\"<CAPTION>\", \"<REASONING>\"]:\n",
    "            if conversation[attr] is not None:\n",
    "                for item in conversation[attr]:\n",
    "                    if item['bbox'] is not None:\n",
    "                        count += 1\n",
    "print(f\"The number of bounding boxes that are not None: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将bbox放回原文本\n",
    "import json\n",
    "import re\n",
    "\n",
    "# 读取两个 JSON 文件\n",
    "with open(file1, 'r') as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "with open(file2, 'r') as f:\n",
    "    data2 = json.load(f)\n",
    "\n",
    "# 遍历第一个 JSON 文件中的每个记录\n",
    "for item1 in data1:\n",
    "    id1 = item1['id']\n",
    "    \n",
    "    # 在第二个 JSON 文件中找到匹配的记录\n",
    "    item2 = next((item for item in data2 if item[\"id\"] == id1), None)\n",
    "    if item2 is None:\n",
    "        continue\n",
    "\n",
    "    # 遍历第一个记录中的 conversations，同时与第二个文件的对话对应\n",
    "    for idx, conversation1 in enumerate(item1['conversations']):\n",
    "        if idx * 2 + 1 >= len(item2['conversations']):\n",
    "            break\n",
    "        \n",
    "        # 获取对应的 from gpt 文本\n",
    "        gpt_response = item2['conversations'][idx * 2 + 1]['value']\n",
    "        \n",
    "        # 处理 <CAPTION>\n",
    "        if \"<CAPTION>\" in conversation1 and conversation1[\"<CAPTION>\"] is not None:\n",
    "            caption_phrases = {phrase[\"content\"]: phrase[\"bbox\"] for phrase in conversation1[\"<CAPTION>\"] if phrase[\"bbox\"] is not None}\n",
    "            caption_content = re.search(r'<CAPTION>(.*?)</CAPTION>', gpt_response, re.DOTALL)\n",
    "            if caption_content:\n",
    "                updated_caption = caption_content.group(1)\n",
    "                for phrase, bbox in caption_phrases.items():\n",
    "                    if phrase in updated_caption:\n",
    "                        updated_caption = updated_caption.replace(phrase, f\"{phrase} {bbox}\")\n",
    "                gpt_response = gpt_response.replace(caption_content.group(1), updated_caption)\n",
    "        \n",
    "        # 处理 <REASONING>\n",
    "        if \"<REASONING>\" in conversation1 and conversation1[\"<REASONING>\"] is not None:\n",
    "            reasoning_phrases = {phrase[\"content\"]: phrase[\"bbox\"] for phrase in conversation1[\"<REASONING>\"] if phrase[\"bbox\"] is not None}\n",
    "            reasoning_content = re.search(r'<REASONING>(.*?)</REASONING>', gpt_response, re.DOTALL)\n",
    "            if reasoning_content:\n",
    "                updated_reasoning = reasoning_content.group(1)\n",
    "                for phrase, bbox in reasoning_phrases.items():\n",
    "                    if phrase in updated_reasoning:\n",
    "                        updated_reasoning = updated_reasoning.replace(phrase, f\"{phrase} {bbox}\")\n",
    "                gpt_response = gpt_response.replace(reasoning_content.group(1), updated_reasoning)\n",
    "\n",
    "        # 更新 gpt_response\n",
    "        item2['conversations'][idx * 2 + 1]['value'] = gpt_response\n",
    "\n",
    "# 输出处理后的 JSON 数据\n",
    "with open('/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/test.json', 'w') as f:\n",
    "    json.dump(data2, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
