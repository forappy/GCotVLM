{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出没有被处理的样本\n",
    "import json\n",
    "\n",
    "# 加载JSON文件\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# 保存数据到JSON文件\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = load_json('/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/trans_14000.json')\n",
    "file2 = load_json('/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/trans_24000.json')\n",
    "file3 = load_json('/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/trans_24000--.json')\n",
    "fileall = file1 + file2 + file3\n",
    "save_json(fileall, '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/trans_all.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27701\n",
      "27701\n"
     ]
    }
   ],
   "source": [
    "# 不完整的处理后的样本\n",
    "file1_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/trans_all.json' \n",
    "# 原本的样本\n",
    "file2_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/sampled_data.json'\n",
    "output_sorted_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/sort_combined_data.json'\n",
    "output_missing_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/unpre_data.json'\n",
    "\n",
    "# 加载两个JSON文件\n",
    "data1 = load_json(file1_path)\n",
    "data2 = load_json(file2_path)\n",
    "print(len(data1))\n",
    "print(len((data2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存排序数据到 /mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/sort_combined_data.json\n",
      "已保存缺失数据到 /mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/unpre_data.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 文件路径替换为你的文件路径\n",
    "# 不完整的处理后的样本\n",
    "file1_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/combined_data.json' \n",
    "# 原本的样本\n",
    "file2_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/fgeo_1_10_gpt_trans.json'\n",
    "output_sorted_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/sort_combined_data.json'\n",
    "output_missing_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/unpre_data.json'\n",
    "\n",
    "# 加载两个JSON文件\n",
    "data1 = load_json(file1_path)\n",
    "data2 = load_json(file2_path)\n",
    "\n",
    "# 为第一个文件的样本创建ID映射\n",
    "data1_dict = {item['id']: item for item in data1}\n",
    "\n",
    "# 按照第二个文件中的顺序重新排列第一个文件中的样本\n",
    "sorted_data1 = [data1_dict[item['id']] for item in data2 if item['id'] in data1_dict]\n",
    "\n",
    "# 找到第一个文件中但第二个文件中不存在的样本\n",
    "missing_data = [item for item in data2 if item['id'] not in set(item['id'] for item in data1)]\n",
    "\n",
    "# 保存结果\n",
    "save_json(sorted_data1, output_sorted_path)\n",
    "save_json(missing_data, output_missing_path)\n",
    "\n",
    "print(f'已保存排序数据到 {output_sorted_path}')\n",
    "print(f'已保存缺失数据到 {output_missing_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 加载JSON文件\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# 保存数据到JSON文件\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# 文件路径替换为你的文件路径\n",
    "# 不完整的处理后的样本\n",
    "file1_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/combined_data.json' \n",
    "# 原本的样本\n",
    "file2_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/fgeo_1_10_gpt_trans.json'\n",
    "output_sorted_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/sort_combined_data.json'\n",
    "output_missing_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/unpre_data.json'\n",
    "\n",
    "# 加载两个JSON文件\n",
    "data1 = load_json(file1_path)\n",
    "data2 = load_json(file2_path)\n",
    "\n",
    "# 为第一个文件的样本创建ID映射\n",
    "data1_dict = {item['id']: item for item in data1}\n",
    "\n",
    "# 按照第二个文件中的顺序重新排列第一个文件中的样本\n",
    "sorted_data1 = [data1_dict[item['id']] for item in data2 if item['id'] in data1_dict]\n",
    "\n",
    "# 找到第一个文件中但第二个文件中不存在的样本\n",
    "missing_data = [item for item in data2 if item['id'] not in set(item['id'] for item in data1)]\n",
    "\n",
    "# 保存结果\n",
    "save_json(sorted_data1, output_sorted_path)\n",
    "save_json(missing_data, output_missing_path)\n",
    "\n",
    "print(f'已保存排序数据到 {output_sorted_path}')\n",
    "print(f'已保存缺失数据到 {output_missing_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27701 70881\n"
     ]
    }
   ],
   "source": [
    "# 合并两个json文件\n",
    "file1_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/sampled_data_ibbox_iou3_nms1.json'\n",
    "file2_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/remaining_data.json'\n",
    "save_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/temp/train-ibbox-iou3_nms1.json'\n",
    "\n",
    "data1 = load_json(file1_path)\n",
    "data2 = load_json(file2_path)\n",
    "print(len(data1), len(data2))\n",
    "conbined_data = data1 + data2\n",
    "\n",
    "save_json(conbined_data, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查预测样本json文件的完整性\n",
    "import json\n",
    "\n",
    "def compare_json_files(file1, file2):\n",
    "    # 读取第一个JSON文件\n",
    "    with open(file1, 'r', encoding='utf-8') as f1:\n",
    "        data1 = json.load(f1)\n",
    "\n",
    "    # 读取第二个JSON文件\n",
    "    with open(file2, 'r', encoding='utf-8') as f2:\n",
    "        data2 = json.load(f2)\n",
    "\n",
    "    # 用字典创建一个快速索引，以便通过id快速访问\n",
    "    data1_index = {item['id']: item for item in data1}\n",
    "    data2_index = {item['id']: item for item in data2}\n",
    "\n",
    "    # 检测样本\"conversations\"长度是否相同\n",
    "    for id_ in data1_index:\n",
    "        if id_ in data2_index:\n",
    "            conv_len1 = len(data1_index[id_].get(\"conversations\", []))\n",
    "            conv_len2 = len(data2_index[id_].get(\"conversations\", []))\n",
    "\n",
    "            if conv_len1 * 2 != conv_len2:\n",
    "                print(f\"Difference found in id {id_}: Length in file1 is {conv_len1}, in file2 is {conv_len2}\")\n",
    "        else:\n",
    "            print(f\"id {id_} not found in file2\")\n",
    "\n",
    "    # 检查file2中有，而file1中没有的id\n",
    "    for id_ in data2_index:\n",
    "        if id_ not in data1_index:\n",
    "            print(f\"id {id_} not found in file1\")\n",
    "file1_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/all-cogvlm.json'\n",
    "file2_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/sampled_data.json'\n",
    "# 示例使用\n",
    "compare_json_files(file1_path, file2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate the Intersection over Union (IoU) of two bounding boxes.\"\"\"\n",
    "    x_left = max(box1[0], box2[0])\n",
    "    y_top = max(box1[1], box2[1])\n",
    "    x_right = min(box1[2], box2[2])\n",
    "    y_bottom = min(box1[3], box2[3])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "    return iou\n",
    "\n",
    "file_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/all-cogvlm.json'\n",
    "with open(file_path, 'r') as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "reference_box = [0, 0, 1, 1]  # The reference box to compare against\n",
    "\n",
    "for sample_data in data1:\n",
    "    # 使用 get 方法处理 conversations\n",
    "    for conversation in sample_data.get(\"conversations\", []):\n",
    "        # 检查 conversation，如果为空字典则初始化\n",
    "        if not conversation or \"<CAPTION>\" not in conversation or \"<REASONING>\" not in conversation:\n",
    "            conversation['<CAPTION>'] = None\n",
    "            conversation['<REASONING>'] = None\n",
    "\n",
    "        # 处理 <CAPTION> 和 <REASONING>\n",
    "        for attr in [\"<CAPTION>\", \"<REASONING>\"]:\n",
    "            items = conversation.get(attr, [])\n",
    "            if items:\n",
    "                for item in items:\n",
    "                    bbox = item.get(\"bbox\", None)\n",
    "                    if bbox is not None:\n",
    "                        iou = calculate_iou(bbox, reference_box)\n",
    "                        if iou > 0.8:\n",
    "                            item[\"bbox\"] = None\n",
    "\n",
    "# 将修改后的数据写回到 JSON 文件\n",
    "output_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/temp/cogvlm-filter1.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(data1, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤3，去除掉flag为0\n",
    "file_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/agent_fego_1_10_bbox2.json'\n",
    "with open(file_path, 'r') as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "for sample_data in data1:\n",
    "    # 使用 get 方法处理 conversations\n",
    "    for conversation in sample_data.get(\"conversations\", []):\n",
    "        # 检查 conversation，如果为空字典则初始化\n",
    "        # if not conversation or \"<CAPTION>\" not in conversation or \"<REASONING>\" not in conversation:\n",
    "        #     conversation['<CAPTION>'] = None\n",
    "        #     conversation['<REASONING>'] = None\n",
    "\n",
    "        # 处理 <CAPTION> 和 <REASONING>\n",
    "        for attr in [\"<CAPTION>\", \"<REASONING>\"]:\n",
    "            items = conversation.get(attr, [])\n",
    "            if items:\n",
    "                for item in items:\n",
    "                    flag = item.get(\"flag\", None)\n",
    "                    if flag == 0:\n",
    "                        item['bbox'] = None\n",
    "                        item['flag'] = 1 \n",
    "\n",
    "# 将修改后的数据写回到 JSON 文件\n",
    "output_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k/json/fego_1_10_bbox3.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(data1, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤4 计算两个模型预测值的iou\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_p, y1_p, x2_p, y2_p = box2\n",
    "    \n",
    "    xi1 = max(x1, x1_p)\n",
    "    yi1 = max(y1, y1_p)\n",
    "    xi2 = min(x2, x2_p)\n",
    "    yi2 = min(y2, y2_p)\n",
    "    \n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    \n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_p - x1_p) * (y2_p - y1_p)\n",
    "    \n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    return inter_area / union_area if union_area != 0 else 0\n",
    "\n",
    "def process_predictions(file1, file2, output_file):\n",
    "    data1 = load_json(file1)\n",
    "    data2 = load_json(file2)\n",
    "\n",
    "    data1_dict = {entry['id']: entry for entry in data1}\n",
    "    data2_dict = {entry['id']: entry for entry in data2}\n",
    "\n",
    "    for entry_id in data1_dict.keys() & data2_dict.keys():\n",
    "        entry1 = data1_dict[entry_id]\n",
    "        entry2 = data2_dict[entry_id]\n",
    "\n",
    "        for convo1, convo2 in zip(entry1.get(\"conversations\", []), entry2.get(\"conversations\", [])):\n",
    "            for tag1, bboxes1 in convo1.items():\n",
    "                bboxes2 = convo2.get(tag1, [])\n",
    "                \n",
    "                if not bboxes1 or not bboxes2:\n",
    "                    continue\n",
    "\n",
    "                for bbox1, bbox2 in zip(bboxes1, bboxes2):\n",
    "                    if not isinstance(bbox1, dict) or not isinstance(bbox2, dict):\n",
    "                        # print(\"Error: bbox is not a dictionary!\")\n",
    "                        continue\n",
    "\n",
    "                    if 'bbox' not in bbox1 or 'bbox' not in bbox2:\n",
    "                        print(\"Error: bbox key not found!\")\n",
    "                        continue\n",
    "\n",
    "                    if bbox1['bbox'] is None or bbox2['bbox'] is None:\n",
    "                        continue\n",
    "                    if bbox1['bbox'] is None or bbox2['bbox'] is None:\n",
    "                        continue\n",
    "                    if bbox1['bbox'] is None or bbox2['bbox'] is None:\n",
    "                        continue\n",
    "\n",
    "                    iou = calculate_iou(bbox1['bbox'], bbox2['bbox'])\n",
    "                    if iou < 0.1:\n",
    "                        # Set the bounding box details to None for low IoU\n",
    "                        bbox1['bbox'] = None\n",
    "                        # bbox1['content'] = None\n",
    "                    # else:\n",
    "                    #     # For IoU >= 0.3, retain the bounding box from model 1 (bbox1)\n",
    "                    #     bbox2['bbox'] = bbox1['bbox']  # Optionally transfer bbox\n",
    "                    #     bbox2['content'] = bbox1['content']  # Optionally transfer content\n",
    "\n",
    "    # Save the modified predictions to a new file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data1, f, indent=4)\n",
    "\n",
    "# File paths\n",
    "path1 = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/temp/ca-filter1.json'\n",
    "path2 = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/temp/cogvlm-filter1.json'\n",
    "output_file = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/ca-iou1.json'\n",
    "\n",
    "# Process predictions and save\n",
    "process_predictions(path1, path2, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of bounding boxes that are not None: 351578\n"
     ]
    }
   ],
   "source": [
    "# 统计有效bbox 原：149284 过滤1:138018 过滤2:136130\n",
    "# cogagent 原：144856 过滤1:132116\n",
    "# iou3:82990 iou4:78631 iou5:74558 iou9:48228\n",
    "# 27000样本中 all-ca:460339 all-cogvlm:478275\n",
    "# 过滤1:      424091         453870\n",
    "# iou3:315923 nms:280955\n",
    "# iou2:331484\n",
    "# iou1:351578\n",
    "import json\n",
    "\n",
    "file_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/ca-iou1.json'\n",
    "\n",
    "# 读取 JSON 文件\n",
    "with open(file_path, 'r') as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "count = 0\n",
    "for sample_data in data1:\n",
    "    # 确保 `conversations` 是一个列表\n",
    "    for conversation in sample_data.get(\"conversations\", []):\n",
    "        # 确保 conversation 是一个字典，并且具备所需的键\n",
    "        if not conversation or \"<CAPTION>\" not in conversation or \"<REASONING>\" not in conversation:\n",
    "            conversation['<CAPTION>'] = None\n",
    "            conversation['<REASONING>'] = None\n",
    "\n",
    "        # 统计不为 None 的 bbox 的数量\n",
    "        for attr in [\"<CAPTION>\", \"<REASONING>\"]:\n",
    "            items = conversation.get(attr, [])\n",
    "            if items:\n",
    "                for item in items:\n",
    "                    if item.get('bbox') is not None:\n",
    "                        count += 1\n",
    "\n",
    "print(f\"The number of bounding boxes that are not None: {count}\")\n",
    "\n",
    "# count = 0\n",
    "# for sample_data in data1:\n",
    "#     for conversation in sample_data[\"conversations\"]:\n",
    "#         if len(conversation) == 0:\n",
    "#             continue\n",
    "#         for attr in [\"<CAPTION>\", \"<REASONING>\"]:\n",
    "#             if conversation[attr] is not None:\n",
    "#                 for item in conversation[attr]:\n",
    "#                     if item['bbox'] is not None:\n",
    "#                         count += 1\n",
    "# print(f\"The number of bounding boxes that are not None: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "image_base_dir = \"/mnt/pfs-mc0p4k/nlu/team/yuhaofu/data/LLaVA-CoT-100k\"\n",
    "file_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/ca-iou1.json'\n",
    "prefix_to_remove = \"/lpai/volumes/ss-nlu-ali-sh/lizr/yuhaofu/data/LLaVa-CoT-100k/\"\n",
    "\n",
    "def num2pixel(bbox, w, h):\n",
    "    if not bbox or len(bbox) != 4:\n",
    "        raise ValueError(\"Bounding box should be a list of 4 elements\")\n",
    "    x0 = bbox[0] * w\n",
    "    y0 = bbox[1] * h\n",
    "    x1 = bbox[2] * w\n",
    "    y1 = bbox[3] * h\n",
    "    return [x0, y0, x1, y1]\n",
    "\n",
    "def scale_and_adjust_bbox(bbox, width, height):\n",
    "    adjusted_bbox = bbox[:]\n",
    "    if width > height:\n",
    "        offset = (width - height) // 2\n",
    "        adjusted_bbox[1] += offset\n",
    "        adjusted_bbox[3] += offset\n",
    "        adjusted_bbox = [x / width for x in adjusted_bbox]\n",
    "    else:\n",
    "        offset = (height - width) // 2\n",
    "        adjusted_bbox[0] += offset\n",
    "        adjusted_bbox[2] += offset\n",
    "        adjusted_bbox = [x / height for x in adjusted_bbox]\n",
    "    # 四舍五入每个值到三位小数\n",
    "    adjusted_bbox = [round(x, 3) for x in adjusted_bbox]\n",
    "    return adjusted_bbox\n",
    "\n",
    "\n",
    "# 读取 JSON 文件\n",
    "with open(file_path, 'r') as f:\n",
    "    data_entries = json.load(f)\n",
    "\n",
    "for entry in data_entries:\n",
    "    if 'image' not in entry:\n",
    "        continue\n",
    "\n",
    "    # Update image path\n",
    "    if entry['image'].startswith(prefix_to_remove):\n",
    "        new_path = entry['image'][len(prefix_to_remove):]\n",
    "    else:\n",
    "        new_path = entry['image']  # 如果路径没有这个前缀，则保持不变\n",
    "\n",
    "    entry['image'] = new_path\n",
    "    image_path = os.path.join(image_base_dir, new_path)\n",
    "\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    except IOError:\n",
    "        continue\n",
    "\n",
    "    w, h = image.size\n",
    "\n",
    "    for conversation in entry.get(\"conversations\", []):\n",
    "        if not conversation or \"<CAPTION>\" not in conversation or \"<REASONING>\" not in conversation:\n",
    "            continue\n",
    "\n",
    "        for attr in [\"<CAPTION>\", \"<REASONING>\"]:\n",
    "            items = conversation.get(attr, [])\n",
    "            if items:\n",
    "                for item in items:\n",
    "                    bbox = item.get(\"bbox\", None)\n",
    "                    if bbox:\n",
    "                        pixel = num2pixel(bbox, w, h)\n",
    "                        scala_bbox = scale_and_adjust_bbox(pixel, w, h)\n",
    "                        item['bbox'] = scala_bbox\n",
    "\n",
    "# 将修改后的数据写回到 JSON 文件\n",
    "output_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/ca-iou1-scale.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(data_entries, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/ca-iou2-scale.json'\n",
    "\n",
    "# 读取 JSON 文件\n",
    "with open(file_path, 'r') as f:\n",
    "    data_entries = json.load(f)\n",
    "\n",
    "def calculate_iou(bbox1, bbox2):\n",
    "    x_min1, y_min1, x_max1, y_max1 = bbox1\n",
    "    x_min2, y_min2, x_max2, y_max2 = bbox2\n",
    "\n",
    "    inter_x_min = max(x_min1, x_min2)\n",
    "    inter_y_min = max(y_min1, y_min2)\n",
    "    inter_x_max = min(x_max1, x_max2)\n",
    "    inter_y_max = min(y_max1, y_max2)\n",
    "\n",
    "    if inter_x_max < inter_x_min or inter_y_max < inter_y_min:\n",
    "        return 0.0\n",
    "\n",
    "    inter_area = (inter_x_max - inter_x_min) * (inter_y_max - inter_y_min)\n",
    "    area1 = (x_max1 - x_min1) * (y_max1 - y_min1)\n",
    "    area2 = (x_max2 - x_min2) * (y_max2 - y_min2)\n",
    "\n",
    "    iou = inter_area / float(area1 + area2 - inter_area)\n",
    "    return iou\n",
    "\n",
    "def area_is_close(bbox1, bbox2, threshold=0.1):\n",
    "    area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
    "    area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
    "    return abs(area1 - area2) / max(area1, area2) < threshold\n",
    "\n",
    "def non_maximum_suppression(items, threshold_iou=0.9, threshold_area=0.1):\n",
    "    for i, ref_item in enumerate(items):\n",
    "        if ref_item[\"bbox\"] is None:\n",
    "            continue\n",
    "\n",
    "        best_item = ref_item\n",
    "        for j, comp_item in enumerate(items):\n",
    "            if j == i or comp_item[\"bbox\"] is None:\n",
    "                continue\n",
    "\n",
    "            if calculate_iou(ref_item[\"bbox\"], comp_item[\"bbox\"]) > threshold_iou and area_is_close(ref_item[\"bbox\"], comp_item[\"bbox\"], threshold_area):\n",
    "                if len(comp_item[\"content\"].split()) > len(best_item[\"content\"].split()):\n",
    "                    best_item = comp_item\n",
    "\n",
    "        # Nullify bboxes of all non-best similar items\n",
    "        for item in items:\n",
    "            if item != best_item and item[\"bbox\"] is not None and calculate_iou(best_item[\"bbox\"], item[\"bbox\"]) > threshold_iou and area_is_close(best_item[\"bbox\"], item[\"bbox\"], threshold_area):\n",
    "                item[\"bbox\"] = None\n",
    "\n",
    "for entry in data_entries:\n",
    "    for conversation in entry.get(\"conversations\", []):\n",
    "        if not conversation or \"<CAPTION>\" not in conversation or \"<REASONING>\" not in conversation:\n",
    "            continue\n",
    "\n",
    "        for attr in [\"<CAPTION>\", \"<REASONING>\"]:\n",
    "            items = conversation.get(attr, [])\n",
    "            if items:\n",
    "                non_maximum_suppression(items)\n",
    "\n",
    "output_file_path = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/temp/ca-iou3-scale-nms.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    json.dump(data_entries, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicate the bounding box coordinates of areas that would assist you in answering the question accurately.\n",
    "When responding to the question, please specify the coordinates of the bounding boxes that contain useful information.\n",
    "Identify and provide the bounding box coordinates for any regions that are relevant to answering the given question.\n",
    "Please specify which regions’ bounding box coordinates are most pertinent to your answer.\n",
    "When answering, highlight the relevant regions by providing their bounding box coordinates.\n",
    "Mark the coordinates of the bounding boxes that are essential for addressing the question.\n",
    "Please indicate the bounding boxes that contain information necessary for answering the question, along with their coordinates.\n",
    "For your response, list the coordinates of all relevant bounding boxes in the image.\n",
    "Please provide the coordinates for the bounding boxes covering regions that support your answer.\n",
    "Indicate which regions’ bounding box coordinates should be considered when forming your answer to the question.\n",
    "Specify the coordinates of any bounding boxes you find helpful for addressing the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将bbox放回原文本\n",
    "import json\n",
    "import re\n",
    "\n",
    "file1 = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/ca-iou1-scale.json'\n",
    "file2 = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/sampled_data.json'\n",
    "\n",
    "# 打开 JSON 文件\n",
    "with open(file1, 'r') as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "with open(file2, 'r') as f:\n",
    "    data2 = json.load(f)\n",
    "\n",
    "# 遍历第一个 JSON 文件中的每个记录\n",
    "for item1 in data1:\n",
    "    id1 = item1['id']\n",
    "    if 'chartqa' in item1['image']:\n",
    "        continue\n",
    "    # 在第二个 JSON 文件中找到匹配的记录\n",
    "    item2 = next((item for item in data2 if item[\"id\"] == id1), None)\n",
    "    if item2 is None:\n",
    "        continue\n",
    "    \n",
    "    # 确保 conversations 的长度遵循约定\n",
    "    assert len(item1['conversations']) * 2 == len(item2['conversations']), (\n",
    "        \"The length of conversations in item1 is not half of that in item2\"\n",
    "    )\n",
    "\n",
    "    # 遍历第一个记录中的 conversations\n",
    "    for idx, conversation1 in enumerate(item1['conversations']):\n",
    "        if idx * 2 + 1 >= len(item2['conversations']):\n",
    "            break\n",
    "        \n",
    "        # 获取相应的 from gpt 文本\n",
    "        gpt_response = item2['conversations'][idx * 2 + 1]['value']\n",
    "        gpt_question = item2['conversations'][idx * 2]['value']\n",
    "        \n",
    "        def replace_first_occurrence(text, phrases_with_bbox):\n",
    "            processed_phrases = set()\n",
    "            for phrase, bbox in phrases_with_bbox.items():\n",
    "                if phrase not in processed_phrases:\n",
    "                    # 使用正则替换完成的单词（词边界 \\b）\n",
    "                    updated_text = re.sub(rf'\\b{re.escape(phrase)}\\b', f\"{phrase} {bbox}\", text, count=1, flags=re.IGNORECASE)\n",
    "                    if updated_text != text:  # 如果进行了替换，则更新记录\n",
    "                        processed_phrases.add(phrase)\n",
    "                    text = updated_text\n",
    "            return text\n",
    "        \n",
    "        # 处理 <CAPTION>\n",
    "        if \"<CAPTION>\" in conversation1 and conversation1[\"<CAPTION>\"] is not None:\n",
    "            caption_phrases = {phrase[\"content\"]: phrase[\"bbox\"] for phrase in conversation1[\"<CAPTION>\"] if phrase[\"bbox\"] is not None}\n",
    "            caption_content = re.search(r'<CAPTION>(.*?)</CAPTION>', gpt_response, re.DOTALL)\n",
    "            if caption_content:\n",
    "                updated_caption = replace_first_occurrence(caption_content.group(1), caption_phrases)\n",
    "                gpt_response = gpt_response.replace(caption_content.group(1), updated_caption)\n",
    "                gpt_question += \" Please provide the bounding box coordinate of the region that can help you answer the question better.\"\n",
    "\n",
    "        # 处理 <REASONING>\n",
    "        if \"<REASONING>\" in conversation1 and conversation1[\"<REASONING>\"] is not None:\n",
    "            reasoning_phrases = {phrase[\"content\"]: phrase[\"bbox\"] for phrase in conversation1[\"<REASONING>\"] if phrase[\"bbox\"] is not None}\n",
    "            reasoning_content = re.search(r'<REASONING>(.*?)</REASONING>', gpt_response, re.DOTALL)\n",
    "            if reasoning_content:\n",
    "                updated_reasoning = replace_first_occurrence(reasoning_content.group(1), reasoning_phrases)\n",
    "                gpt_response = gpt_response.replace(reasoning_content.group(1), updated_reasoning)\n",
    "\n",
    "        # 更新 gpt_response\n",
    "        item2['conversations'][idx * 2]['value'] = gpt_question\n",
    "        item2['conversations'][idx * 2 + 1]['value'] = gpt_response\n",
    "\n",
    "# 保存到新的 JSON 文件\n",
    "with open('/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/sampled_data_ibbox_iou1.json', 'w') as f:\n",
    "    json.dump(data2, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 放松对反斜杠和引号的限制\n",
    "import json\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    # 去除引号、反斜杠，转换为小写，并去除多余空格\n",
    "    return text.replace('\\\"', '').replace('\\'', '').replace('\\\\', '').strip().lower()\n",
    "\n",
    "file1 = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/temp/ca-iou3-scale-nms.json'\n",
    "file2 = '/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/sampled_data.json'\n",
    "\n",
    "# 读取 JSON 文件\n",
    "with open(file1, 'r') as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "with open(file2, 'r') as f:\n",
    "    data2 = json.load(f)\n",
    "\n",
    "# 遍历第一个 JSON 文件中的每个记录\n",
    "for item1 in data1:\n",
    "    id1 = item1['id']\n",
    "    if 'chartqa' in item1['image']:\n",
    "        continue\n",
    "\n",
    "    # 在第二个 JSON 文件中找到匹配的记录\n",
    "    item2 = next((item for item in data2 if item[\"id\"] == id1), None)\n",
    "    if item2 is None:\n",
    "        continue\n",
    "\n",
    "    # 验证 conversations 的长度是否符合我们的假设\n",
    "    assert len(item1['conversations']) * 2 == len(item2['conversations']), (\n",
    "        \"The length of conversations in item1 is not half of that in item2\"\n",
    "    )\n",
    "\n",
    "    # 遍历第一个记录中的 conversations\n",
    "    for idx, conversation1 in enumerate(item1['conversations']):\n",
    "        if idx * 2 + 1 >= len(item2['conversations']):\n",
    "            break\n",
    "        \n",
    "        # 获取相应的 gpt 文本\n",
    "        gpt_response = item2['conversations'][idx * 2 + 1]['value']\n",
    "        gpt_question = item2['conversations'][idx * 2]['value']\n",
    "        \n",
    "        def replace_first_occurrence(text, phrases_with_bbox):\n",
    "            processed_phrases = set()\n",
    "            normalized_text = normalize_text(text)  # 规格化整个文本\n",
    "            for phrase, bbox in phrases_with_bbox.items():\n",
    "                norm_phrase = normalize_text(phrase)\n",
    "                if norm_phrase not in processed_phrases:\n",
    "                    # 使用正则替换规格化后的文本（忽略大小写）\n",
    "                    updated_text = re.sub(rf'\\b{re.escape(norm_phrase)}\\b', f\"{phrase} {bbox}\", normalized_text, count=1, flags=re.IGNORECASE)\n",
    "                    if updated_text != normalized_text:  # 如果进行了替换，则标记为已处理\n",
    "                        processed_phrases.add(norm_phrase)\n",
    "                    normalized_text = updated_text\n",
    "            return normalized_text\n",
    "        \n",
    "        # 处理 <CAPTION> 部分\n",
    "        if \"<CAPTION>\" in conversation1 and conversation1[\"<CAPTION>\"] is not None:\n",
    "            caption_phrases = {phrase[\"content\"]: phrase[\"bbox\"] for phrase in conversation1[\"<CAPTION>\"] if phrase[\"bbox\"] is not None}\n",
    "            caption_content = re.search(r'<CAPTION>(.*?)</CAPTION>', gpt_response, re.DOTALL)\n",
    "            if caption_content:\n",
    "                updated_caption = replace_first_occurrence(caption_content.group(1), caption_phrases)\n",
    "                gpt_response = gpt_response.replace(caption_content.group(1), updated_caption)\n",
    "                gpt_question += \" Please provide the bounding box coordinate of the region that can help you answer the question better.\"\n",
    "\n",
    "        # 处理 <REASONING> 部分\n",
    "        if \"<REASONING>\" in conversation1 and conversation1[\"<REASONING>\"] is not None:\n",
    "            reasoning_phrases = {phrase[\"content\"]: phrase[\"bbox\"] for phrase in conversation1[\"<REASONING>\"] if phrase[\"bbox\"] is not None}\n",
    "            reasoning_content = re.search(r'<REASONING>(.*?)</REASONING>', gpt_response, re.DOTALL)\n",
    "            if reasoning_content:\n",
    "                updated_reasoning = replace_first_occurrence(reasoning_content.group(1), reasoning_phrases)\n",
    "                gpt_response = gpt_response.replace(reasoning_content.group(1), updated_reasoning)\n",
    "\n",
    "        # 更新 gpt_response\n",
    "        item2['conversations'][idx * 2]['value'] = gpt_question\n",
    "        item2['conversations'][idx * 2 + 1]['value'] = gpt_response\n",
    "\n",
    "# 保存到新的 JSON 文件\n",
    "with open('/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/sampled_data_ibbox_iou3_nms1.json', 'w') as f:\n",
    "    json.dump(data2, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def shuffle_json_file(input_file_path, output_file_path):\n",
    "    # 读取JSON文件\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # 确保数据是列表\n",
    "    if isinstance(data, list):\n",
    "        # 打乱列表\n",
    "        random.shuffle(data)\n",
    "    else:\n",
    "        raise ValueError(\"The JSON file must contain a top-level list.\")\n",
    "\n",
    "    # 保存打乱后的数据\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "\n",
    "# 使用示例\n",
    "input_path = \"/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/temp/train-ibbox-iou3_nms1.json\"\n",
    "shuffle_path = \"/mnt/pfs-mc0p4k/nlu/team/yuhaofu/model/GCotVLM/GCot_json/shuffle-train-ibbox-iou3_nms1.json\"\n",
    "shuffle_json_file(input_path, shuffle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a busy street [0.0, 0.405, 0.998, 0.705]\n",
      "the traffic light [0.655, 0.259, 0.702, 0.29]\n",
      "<SUMMARY> I will analyze the image to determine what the bus is doing by observing its position on the road and any visual cues related to its movement. I will then use this information to match the activity with the provided options. </SUMMARY>\n",
      "\n",
      "<CAPTION> The image shows a view from inside a bus looking out onto a busy street [0.0, 0.405, 0.998, 0.705]. The bus is positioned in a lane with multiple vehicles ahead, including another bus. the traffic light [0.655, 0.259, 0.702, 0.29] appears to be red, indicating a stop. </CAPTION>\n",
      "\n",
      "<REASONING> The bus shown in the image is in a lane with other vehicles, facing a red traffic light. This suggests that it is at or nearing a stop. Given the traffic light and the presence of other vehicles waiting, it's logical to deduce that the bus is slowing down to stop at the light, rather than turning, speeding, or backing up. </REASONING>\n",
      "\n",
      "<CONCLUSION> B </CONCLUSION>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "conversation1 = {\n",
    "        \"<CAPTION>\": [\n",
    "          {\n",
    "            \"content\": \"desert food web\",\n",
    "            \"bbox\": None\n",
    "          },\n",
    "          {\n",
    "            \"content\": \"hawk\",\n",
    "            \"bbox\": [\n",
    "              0.262,\n",
    "              0.095,\n",
    "              0.428,\n",
    "              0.228\n",
    "            ]\n",
    "          },\n",
    "          {\n",
    "            \"content\": \"fox\",\n",
    "            \"bbox\": [\n",
    "              0.742,\n",
    "              0.06,\n",
    "              0.902,\n",
    "              0.238\n",
    "            ]\n",
    "          },\n",
    "          {\n",
    "            \"content\": \"lizard\",\n",
    "            \"bbox\": [\n",
    "              0.468,\n",
    "              0.644,\n",
    "              0.622,\n",
    "              0.701\n",
    "            ]\n",
    "          }\n",
    "        ],\n",
    "        \"<REASONING>\": [\n",
    "          {\n",
    "            \"content\": \"hawk\",\n",
    "            \"bbox\": [\n",
    "              0.262,\n",
    "              0.095,\n",
    "              0.428,\n",
    "              0.228\n",
    "            ]\n",
    "          },\n",
    "          {\n",
    "            \"content\": \"fox\",\n",
    "            \"bbox\": [\n",
    "              0.742,\n",
    "              0.06,\n",
    "              0.902,\n",
    "              0.238\n",
    "            ]\n",
    "          },\n",
    "          {\n",
    "            \"content\": \"lizard\",\n",
    "            \"bbox\": [\n",
    "              0.468,\n",
    "              0.644,\n",
    "              0.622,\n",
    "              0.701\n",
    "            ]\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    \n",
    "conversation2 = [\n",
    "            {\n",
    "                \"from\": \"human\",\n",
    "                \"value\": \"How many predators does the lizard have?4\\n2\\n5\\n1 Please answer the question based on the options mentioned before.\"\n",
    "            },\n",
    "            {\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\": \"<SUMMARY> I will approach the problem by first identifying the lizard in the food web image and then counting how many predators are connected to it. I will follow the connections to see which animals feed on the lizard and determine the number of predators it has. </SUMMARY>\\n\\n<CAPTION> The image depicts a desert food web. It includes various animals like a hawk, fox, snake, tarantula, scorpion, lizard, insects, and rodents. The lines between the animals show predator-prey relationships, indicating who eats whom. The lizard is situated in the middle of the food web. </CAPTION>\\n\\n<REASONING> To determine the number of predators the lizard has, I will look at the lines pointing towards the lizard. These lines will show which animals consider the lizard as prey. In the food web: \\n- The hawk has a line leading to the lizard, signifying it preys on the lizard.\\n- The fox also has a line leading to the lizard, indicating it preys on the lizard.\\nWith these observations, the lizard has two predators. </REASONING>\\n\\n<CONCLUSION> 2 </CONCLUSION>\"\n",
    "            }\n",
    "        ]\n",
    "gpt_response = \"<SUMMARY> I will approach the problem by first identifying the lizard in the food web image and then counting how many predators are connected to it. I will follow the connections to see which animals feed on the lizard and determine the number of predators it has. </SUMMARY>\\n\\n<CAPTION> The image depicts a desert food web. It includes various animals like a hawk, fox, snake, tarantula, scorpion, lizard, insects, and rodents. The lines between the animals show predator-prey relationships, indicating who eats whom. The lizard is situated in the middle of the food web. </CAPTION>\\n\\n<REASONING> To determine the number of predators the lizard has, I will look at the lines pointing towards the lizard. These lines will show which animals consider the lizard as prey. In the food web: \\n- The hawk has a line leading to the lizard, signifying it preys on the lizard.\\n- The fox also has a line leading to the lizard, indicating it preys on the lizard.\\nWith these observations, the lizard has two predators. </REASONING>\\n\\n<CONCLUSION> 2 </CONCLUSION>\"\n",
    "\n",
    "\n",
    "conversation1 = {\n",
    "        \"<CAPTION>\": [\n",
    "          {\n",
    "            \"content\": \"a bus\",\n",
    "            \"bbox\": None\n",
    "          },\n",
    "          {\n",
    "            \"content\": \"a busy street\",\n",
    "            \"bbox\": [\n",
    "              0.0,\n",
    "              0.405,\n",
    "              0.998,\n",
    "              0.705\n",
    "            ]\n",
    "          },\n",
    "          {\n",
    "            \"content\": \"multiple vehicles\",\n",
    "            \"bbox\": None\n",
    "          },\n",
    "          {\n",
    "            \"content\": \"the traffic light\",\n",
    "            \"bbox\": [\n",
    "              0.655,\n",
    "              0.259,\n",
    "              0.702,\n",
    "              0.29\n",
    "            ]\n",
    "          }\n",
    "        ],\n",
    "        \"<REASONING>\": [\n",
    "          {\n",
    "            \"content\": \"the bus\",\n",
    "            \"bbox\": None\n",
    "          },\n",
    "          {\n",
    "            \"content\": \"a red traffic light\",\n",
    "            \"bbox\": None\n",
    "          },\n",
    "          {\n",
    "            \"content\": \"other vehicles\",\n",
    "            \"bbox\": None\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "\n",
    "gpt_response = \"<SUMMARY> I will analyze the image to determine what the bus is doing by observing its position on the road and any visual cues related to its movement. I will then use this information to match the activity with the provided options. </SUMMARY>\\n\\n<CAPTION> The image shows a view from inside a bus looking out onto a busy street. The bus is positioned in a lane with multiple vehicles ahead, including another bus. The traffic light appears to be red, indicating a stop. </CAPTION>\\n\\n<REASONING> The bus shown in the image is in a lane with other vehicles, facing a red traffic light. This suggests that it is at or nearing a stop. Given the traffic light and the presence of other vehicles waiting, it's logical to deduce that the bus is slowing down to stop at the light, rather than turning, speeding, or backing up. </REASONING>\\n\\n<CONCLUSION> B </CONCLUSION>\"\n",
    "      \n",
    "# 遍历第一个记录中的 conversations\n",
    "\n",
    "# 获取相应的 from gpt 文本\n",
    "\n",
    "\n",
    "def replace_first_occurrence(text, phrases_with_bbox):\n",
    "    processed_phrases = set()\n",
    "    for phrase, bbox in phrases_with_bbox.items():\n",
    "        if phrase not in processed_phrases:\n",
    "            # 使用正则替换完成的单词（词边界 \\b）\n",
    "            print(phrase, bbox)\n",
    "            updated_text = re.sub(rf'\\b{re.escape(phrase)}\\b', f\"{phrase} {bbox}\", text, count=1, flags=re.IGNORECASE)\n",
    "            # print(updated_text)\n",
    "            if updated_text != text:  # 如果进行了替换，则更新记录\n",
    "                processed_phrases.add(phrase)\n",
    "            text = updated_text\n",
    "    return text\n",
    "\n",
    "# 处理 <CAPTION>\n",
    "if \"<CAPTION>\" in conversation1 and conversation1[\"<CAPTION>\"] is not None:\n",
    "    caption_phrases = {phrase[\"content\"]: phrase[\"bbox\"] for phrase in conversation1[\"<CAPTION>\"] if phrase[\"bbox\"] is not None}\n",
    "    caption_content = re.search(r'<CAPTION>(.*?)</CAPTION>', gpt_response, re.DOTALL)\n",
    "    if caption_content:\n",
    "        updated_caption = replace_first_occurrence(caption_content.group(1), caption_phrases)\n",
    "        gpt_response = gpt_response.replace(caption_content.group(1), updated_caption)\n",
    "\n",
    "# 处理 <REASONING>\n",
    "if \"<REASONING>\" in conversation1 and conversation1[\"<REASONING>\"] is not None:\n",
    "    reasoning_phrases = {phrase[\"content\"]: phrase[\"bbox\"] for phrase in conversation1[\"<REASONING>\"] if phrase[\"bbox\"] is not None}\n",
    "    reasoning_content = re.search(r'<REASONING>(.*?)</REASONING>', gpt_response, re.DOTALL)\n",
    "    if reasoning_content:\n",
    "        updated_reasoning = replace_first_occurrence(reasoning_content.group(1), reasoning_phrases)\n",
    "        gpt_response = gpt_response.replace(reasoning_content.group(1), updated_reasoning)\n",
    "\n",
    "print(gpt_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
